# AI Observability Summarizer - Application Components
# These are deployed per-namespace (each CR gets its own instance)

# =============================================================================
# ALWAYS INSTALLED COMPONENTS (Core functionality, always deployed)
# =============================================================================

# MCP Server - Model Context Protocol server for AI observability
mcpServer:
  enabled: true  # Always deployed - do not change
  # replicaCount: 1
  # image:
  #   repository: quay.io/ecosystem-appeng/aiobs-mcp-server
  #   tag: latest
  # env:
  #   KORREL8R_ENABLED: "true"

# OpenShift Console Plugin - Native console integration
consolePlugin:
  enabled: true  # Always deployed - do not change
  plugin:
    name: "openshift-ai-observability"
    description: "AI Observability Plugin"
    image: "quay.io/ecosystem-appeng/aiobs-console-plugin:latest"
    imagePullPolicy: Always
    replicas: 2
    port: 9443
    # securityContext:
    #   enabled: true
    # resources:
    #   requests:
    #     cpu: 10m
    #     memory: 50Mi

# =============================================================================
# OPTIONAL COMPONENTS (Toggle via OLM form or CR spec)
# =============================================================================

# Alerting - Alert processing CronJob
alerting:
  enabled: false  # Disabled by default - toggle in OLM form
  # image:
  #   repository: quay.io/ecosystem-appeng/aiobs-metrics-alerting
  #   tag: latest

# RAG - LLM Stack (llama-stack, llm-service, pgvector)
rag:
  enabled: true  # Always enabled
  
  # Model selection - Enable ONE model via OLM UI toggle
  global:
    models:
      # Default: Llama 3.1 8B (RECOMMENDED, 16GB GPU)
      llama-3-1-8b-instruct:
        enabled: true
  
  # LLM Service configuration
  llm-service:
    # Device type (set via OLM UI)
    device: gpu  # Options: gpu, hpu, gpu-amd, cpu
    
    # HuggingFace token for model download
    secret:
      hf_token: ""  # Required - set via OLM UI

